# Big Data Spark Analysis

This project demonstrates how to use Apache Spark for large-scale data processing. The focus is on analyzing a heart failure dataset using PySpark in a distributed environment.

## 🔧 Technologies Used
- Python
- Apache Spark
- PySpark
- Google Colab (for execution)
- GraphFrames (optional)

## 📁 Dataset
The dataset used is `heart.csv`, which contains patient health records related to heart failure conditions. It includes variables like age, blood pressure, serum creatinine, and more.

## 📊 What This Notebook Covers
- Setting up a Spark environment with PySpark
- Reading and exploring the dataset using Spark DataFrames
- Cleaning data (duplicate removal, null checks)
- Basic statistical insights using PySpark functions

## 📌 Highlights
- Efficient data loading with Spark
- Hands-on practice with distributed DataFrames
- Useful starter template for anyone new to Spark or PySpark

## 🚀 How to Run
1. Open the notebook in [Google Colab](https://colab.research.google.com/)
2. Upload your `heart.csv` dataset
3. Run the cells sequentially

## 📎 Author
Sravani Elavarthi  
Master’s in Data Science  
University of Maryland, College Park
