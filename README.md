# Big Data Spark Analysis

This project demonstrates how to use Apache Spark for large-scale data processing. The focus is on analyzing a heart failure dataset using PySpark in a distributed environment.

## ğŸ”§ Technologies Used
- Python
- Apache Spark
- PySpark
- Google Colab (for execution)
- GraphFrames (optional)

## ğŸ“ Dataset
The dataset used is `heart.csv`, which contains patient health records related to heart failure conditions. It includes variables like age, blood pressure, serum creatinine, and more.

## ğŸ“Š What This Notebook Covers
- Setting up a Spark environment with PySpark
- Reading and exploring the dataset using Spark DataFrames
- Cleaning data (duplicate removal, null checks)
- Basic statistical insights using PySpark functions

## ğŸ“Œ Highlights
- Efficient data loading with Spark
- Hands-on practice with distributed DataFrames
- Useful starter template for anyone new to Spark or PySpark

## ğŸš€ How to Run
1. Open the notebook in [Google Colab](https://colab.research.google.com/)
2. Upload your `heart.csv` dataset
3. Run the cells sequentially

## ğŸ“ Author
Sravani Elavarthi  
Masterâ€™s in Data Science  
University of Maryland, College Park
